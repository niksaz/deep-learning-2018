{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word vectors in spaCy\n",
    "\n",
    "Okay, let's have some fun with real word vectors. We're going to use the GloVe vectors that come with spaCy to creatively analyze and manipulate the text of Bram Stoker's *Dracula*. First, make sure you've got `spacy` imported:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell loads the language model and parses the input text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "doc = nlp(open(\"pg345.txt\").read().decode('utf8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the cell below creates a list of unique words (or tokens) in the text, as a list of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# all of the words in the text file\n",
    "tokens = list(set([w.text for w in doc if w.is_alpha]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the vector of any word in spaCy's vocabulary using the `vocab` attribute, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -5.52519977e-01,   1.88940004e-01,   6.87370002e-01,\n",
       "        -1.97889999e-01,   7.05749989e-02,   1.00750005e+00,\n",
       "         5.17890006e-02,  -1.56029999e-01,   3.19409996e-01,\n",
       "         1.17019999e+00,  -4.72479999e-01,   4.28669989e-01,\n",
       "        -4.20249999e-01,   2.48030007e-01,   6.81940019e-01,\n",
       "        -6.74880028e-01,   9.24009979e-02,   1.30890000e+00,\n",
       "        -3.62779982e-02,   2.00979993e-01,   7.60049999e-01,\n",
       "        -6.67179972e-02,  -7.77940005e-02,   2.38440007e-01,\n",
       "        -2.43509993e-01,  -5.41639984e-01,  -3.35399985e-01,\n",
       "         2.98049986e-01,   3.52690011e-01,  -8.05939972e-01,\n",
       "        -4.36109990e-01,   6.15350008e-01,   3.42119992e-01,\n",
       "        -3.36030006e-01,   3.32819998e-01,   3.80650014e-01,\n",
       "         5.74270003e-02,   9.99180004e-02,   1.25249997e-01,\n",
       "         1.10389996e+00,   3.66780013e-02,   3.04899991e-01,\n",
       "        -1.49419993e-01,   3.29120010e-01,   2.32999995e-01,\n",
       "         4.33950007e-01,   1.56660005e-01,   2.27779999e-01,\n",
       "        -2.58300006e-02,   2.43340001e-01,  -5.81360012e-02,\n",
       "        -1.34859994e-01,   2.45210007e-01,  -3.34589988e-01,\n",
       "         4.28389996e-01,  -4.81810004e-01,   1.34029999e-01,\n",
       "         2.60490000e-01,   8.99330005e-02,  -9.37699974e-02,\n",
       "         3.76720011e-01,  -2.95579992e-02,   4.38410014e-01,\n",
       "         6.12119973e-01,  -2.57200003e-01,  -7.85059988e-01,\n",
       "         2.38800004e-01,   1.33990005e-01,  -7.93149993e-02,\n",
       "         7.05820024e-01,   3.99679989e-01,   6.77789986e-01,\n",
       "        -2.04739999e-03,   1.97850000e-02,  -4.20590013e-01,\n",
       "        -5.38580000e-01,  -5.21549992e-02,   1.72519997e-01,\n",
       "         2.75469989e-01,  -4.44819987e-01,   2.35949993e-01,\n",
       "        -2.34449998e-01,   3.01030010e-01,  -5.50960004e-01,\n",
       "        -3.11590005e-02,  -3.44330013e-01,   1.23860002e+00,\n",
       "         1.03170002e+00,  -2.27280006e-01,  -9.52070020e-03,\n",
       "        -2.54319996e-01,  -2.97919989e-01,   2.59339988e-01,\n",
       "        -1.04209997e-01,  -3.38759989e-01,   4.24699992e-01,\n",
       "         5.83350018e-04,   1.30930007e-01,   2.87860006e-01,\n",
       "         2.34740004e-01,   2.59050000e-02,  -6.43589973e-01,\n",
       "         6.13300018e-02,   6.38419986e-01,   1.47049993e-01,\n",
       "        -6.15939975e-01,   2.50970006e-01,  -4.48720008e-01,\n",
       "         8.68250012e-01,   9.95550007e-02,  -4.47340012e-02,\n",
       "        -7.42389977e-01,  -5.91470003e-01,  -5.49290001e-01,\n",
       "         3.81080002e-01,   5.51769994e-02,  -1.04869999e-01,\n",
       "        -1.28380001e-01,   6.05210010e-03,   2.87429988e-01,\n",
       "         2.15920001e-01,   7.28709996e-02,  -3.16439986e-01,\n",
       "        -4.33209985e-01,   1.86820000e-01,   6.72739968e-02,\n",
       "         2.81150013e-01,  -4.62220013e-02,  -9.68030021e-02,\n",
       "         5.60909986e-01,  -6.77619994e-01,  -1.66449994e-01,\n",
       "         1.55530006e-01,   5.23010015e-01,  -3.00579995e-01,\n",
       "        -3.72909993e-01,   8.78949985e-02,  -1.79629996e-01,\n",
       "        -4.41929996e-01,  -4.46069986e-01,  -2.41219997e+00,\n",
       "         3.37379992e-01,   6.24159992e-01,   4.27870005e-01,\n",
       "        -2.53859997e-01,  -6.16829991e-01,  -7.00969994e-01,\n",
       "         4.93030012e-01,   3.69159997e-01,  -9.74989980e-02,\n",
       "         6.14109993e-01,  -4.75719990e-03,   4.39159989e-01,\n",
       "        -2.15509996e-01,  -5.67449987e-01,  -4.02779996e-01,\n",
       "         2.94589996e-01,  -3.08499992e-01,   1.01030000e-01,\n",
       "         7.97410011e-02,  -6.38109982e-01,   2.47810006e-01,\n",
       "        -4.45459992e-01,   1.08280003e-01,  -2.36240000e-01,\n",
       "        -5.08379996e-01,  -1.70010000e-01,  -7.87349999e-01,\n",
       "         3.40730011e-01,  -3.18300009e-01,   4.52859998e-01,\n",
       "        -9.51180011e-02,   2.07719997e-01,  -8.01829994e-02,\n",
       "        -3.79819989e-01,  -4.99489993e-01,   4.07590009e-02,\n",
       "        -3.77240002e-01,  -8.97049978e-02,  -6.81869984e-01,\n",
       "         2.21059993e-01,  -3.99309993e-01,   3.23289990e-01,\n",
       "        -3.61799985e-01,  -7.20929980e-01,  -6.34039998e-01,\n",
       "         4.31250006e-01,  -4.97429997e-01,  -1.73950002e-01,\n",
       "        -3.87789994e-01,  -3.25560004e-01,   1.44229993e-01,\n",
       "        -8.34010020e-02,  -2.29939997e-01,   2.77929991e-01,\n",
       "         4.91120011e-01,   6.45110011e-01,  -7.89450034e-02,\n",
       "         1.11709997e-01,   3.72640014e-01,   1.30700007e-01,\n",
       "        -6.16069995e-02,  -4.35009986e-01,   2.89990008e-02,\n",
       "         5.62240005e-01,   5.80120012e-02,   4.70779985e-02,\n",
       "         4.27700013e-01,   7.32450008e-01,  -2.11500004e-02,\n",
       "         1.19879998e-01,   7.88230002e-02,  -1.91060007e-01,\n",
       "         3.52779999e-02,  -3.11019987e-01,   1.32090002e-01,\n",
       "        -2.86060005e-01,  -1.56489998e-01,  -6.43390000e-01,\n",
       "         4.45989996e-01,  -3.09119999e-01,   4.45199996e-01,\n",
       "        -3.67740005e-01,   2.73270011e-01,   6.78330004e-01,\n",
       "        -8.38299990e-02,  -4.51200008e-01,   1.07539997e-01,\n",
       "        -4.59080011e-01,   1.50950000e-01,  -4.58559990e-01,\n",
       "         3.44650000e-01,   7.80130029e-02,  -2.83190012e-01,\n",
       "        -2.81489994e-02,   2.44039997e-01,  -7.13450015e-01,\n",
       "         5.28340004e-02,  -2.80849993e-01,   2.53439993e-02,\n",
       "         4.29789983e-02,   1.56629995e-01,  -7.46469975e-01,\n",
       "        -1.13010001e+00,   4.41350013e-01,   3.14440012e-01,\n",
       "        -1.00180000e-01,  -5.35260022e-01,  -9.06009972e-01,\n",
       "        -6.49540007e-01,   4.26639989e-02,  -7.99269974e-02,\n",
       "         3.29050004e-01,  -3.07969987e-01,  -1.91900004e-02,\n",
       "         4.27650005e-01,   3.14599991e-01,   2.90509999e-01,\n",
       "        -2.73860008e-01,   6.84830010e-01,   1.93949994e-02,\n",
       "        -3.28839988e-01,  -4.82389987e-01,  -1.57470003e-01,\n",
       "        -1.60359994e-01,   4.91640002e-01,  -7.03520000e-01,\n",
       "        -3.55910003e-01,  -7.48870015e-01,  -5.28270006e-01,\n",
       "         4.49829996e-02,   5.92469983e-02,   4.62240010e-01,\n",
       "         8.96970034e-02,  -7.56179988e-01,   6.36820018e-01,\n",
       "         9.06800032e-02,   6.88299984e-02,   1.82960004e-01,\n",
       "         1.07539997e-01,   6.78110003e-01,  -1.47159994e-01,\n",
       "         1.70289993e-01,  -5.26300013e-01,   1.92680001e-01,\n",
       "         9.31299984e-01,   8.03629994e-01,   6.13240004e-01,\n",
       "        -3.04939985e-01,   2.02360004e-01,   5.85200012e-01,\n",
       "         2.64840007e-01,  -4.58629996e-01,   2.10350007e-03,\n",
       "        -5.69899976e-01,  -4.90920007e-01,   4.25110012e-01,\n",
       "        -1.09539998e+00,   1.71240002e-01,   2.24950001e-01], dtype=float32)"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab['cheese'].vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of convenience, the following function gets the vector of a given string from spaCy's vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vec(s):\n",
    "    return nlp.vocab[s].vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine similarity and finding closest neighbors\n",
    "\n",
    "The cell below defines a function `cosine()`, which returns the [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity) of two vectors. Cosine similarity is another way of determining how similar two vectors are, which is more suited to high-dimensional spaces. [See the Encyclopedia of Distances for more information and even more ways of determining vector similarity.](http://www.uco.es/users/ma1fegan/Comunes/asignaturas/vision/Encyclopedia-of-distances-2009.pdf)\n",
    "\n",
    "(You'll need to install `numpy` to get this to work. If you haven't already: `pip install numpy`. Use `sudo` if you need to and make sure you've upgraded to the most recent version of `pip` with `sudo pip install --upgrade pip`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "# cosine similarity\n",
    "def cosine(v1, v2):\n",
    "    if norm(v1) > 0 and norm(v2) > 0:\n",
    "        return dot(v1, v2) / (norm(v1) * norm(v2))\n",
    "    else:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell shows that the cosine similarity between `dog` and `puppy` is larger than the similarity between `trousers` and `octopus`, thereby demonstrating that the vectors are working how we expect them to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine(vec('dog'), vec('puppy')) > cosine(vec('trousers'), vec('octopus'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell defines a function that iterates through a list of tokens and returns the token whose vector is most similar to a given vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def spacy_closest(token_list, vec_to_check, n=10):\n",
    "    return sorted(token_list,\n",
    "                  key=lambda x: cosine(vec_to_check, vec(x)),\n",
    "                  reverse=True)[:n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this function, we can get a list of synonyms, or words closest in meaning (or distribution, depending on how you look at it), to any arbitrary word in spaCy's vocabulary. In the following example, we're finding the words in *Dracula* closest to \"basketball\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'tennis',\n",
       " u'coach',\n",
       " u'game',\n",
       " u'teams',\n",
       " u'Junior',\n",
       " u'junior',\n",
       " u'Team',\n",
       " u'school',\n",
       " u'boys',\n",
       " u'leagues']"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what's the closest equivalent of basketball?\n",
    "spacy_closest(tokens, vec(\"basketball\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fun with spaCy, Dracula, and vector arithmetic\n",
    "\n",
    "Now we can start doing vector arithmetic and finding the closest words to the resulting vectors. For example, what word is closest to the halfway point between day and night?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'night',\n",
       " u'day',\n",
       " u'Day',\n",
       " u'evening',\n",
       " u'Evening',\n",
       " u'Morning',\n",
       " u'morning',\n",
       " u'afternoon',\n",
       " u'Nights',\n",
       " u'nights']"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# halfway between day and night\n",
    "spacy_closest(tokens, meanv([vec(\"day\"), vec(\"night\")]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variations of `night` and `day` are still closest, but after that we get words like `evening` and `morning`, which are indeed halfway between day and night!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the closest words in _Dracula_ to \"wine\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'wine',\n",
       " u'beer',\n",
       " u'bottle',\n",
       " u'Drink',\n",
       " u'drink',\n",
       " u'fruit',\n",
       " u'bottles',\n",
       " u'taste',\n",
       " u'coffee',\n",
       " u'tasted']"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_closest(tokens, vec(\"wine\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you subtract \"alcohol\" from \"wine\" and find the closest words to the resulting vector, you're left with simply a lovely dinner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'wine',\n",
       " u'Dinner',\n",
       " u'dinner',\n",
       " u'lovely',\n",
       " u'delicious',\n",
       " u'salad',\n",
       " u'treasure',\n",
       " u'wonderful',\n",
       " u'Wonderful',\n",
       " u'cheese']"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_closest(tokens, subtractv(vec(\"wine\"), vec(\"alcohol\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The closest words to \"water\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'water',\n",
       " u'waters',\n",
       " u'salt',\n",
       " u'Salt',\n",
       " u'dry',\n",
       " u'liquid',\n",
       " u'ocean',\n",
       " u'boiling',\n",
       " u'heat',\n",
       " u'sand']"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_closest(tokens, vec(\"water\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if you add \"frozen\" to \"water,\" you get \"ice\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'water',\n",
       " u'cold',\n",
       " u'ice',\n",
       " u'salt',\n",
       " u'Salt',\n",
       " u'dry',\n",
       " u'fresh',\n",
       " u'liquid',\n",
       " u'boiling',\n",
       " u'milk']"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_closest(tokens, addv(vec(\"water\"), vec(\"frozen\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can even do analogies! For example, the words most similar to \"grass\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'grass',\n",
       " u'lawn',\n",
       " u'trees',\n",
       " u'garden',\n",
       " u'GARDEN',\n",
       " u'sand',\n",
       " u'tree',\n",
       " u'soil',\n",
       " u'Green',\n",
       " u'green']"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_closest(tokens, vec(\"grass\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you take the difference of \"blue\" and \"sky\" and add it to grass, you get the analogous word (\"green\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'grass',\n",
       " u'Green',\n",
       " u'green',\n",
       " u'GREEN',\n",
       " u'yellow',\n",
       " u'red',\n",
       " u'Red',\n",
       " u'purple',\n",
       " u'lawn',\n",
       " u'pink']"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# analogy: blue is to sky as X is to grass\n",
    "blue_to_sky = subtractv(vec(\"blue\"), vec(\"sky\"))\n",
    "spacy_closest(tokens, addv(blue_to_sky, vec(\"grass\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the vector for a sentence, we simply average its component vectors, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentvec(s):\n",
    "    sent = nlp(s)\n",
    "    return meanv([w.vector for w in sent])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find the sentence in our text file that is closest in \"meaning\" to an arbitrary input sentence. First, we'll get the list of sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = list(doc.sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function takes a list of sentences from a spaCy parse and compares them to an input sentence, sorting them by cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def spacy_closest_sent(space, input_str, n=10):\n",
    "    input_vec = sentvec(input_str)\n",
    "    return sorted(space,\n",
    "                  key=lambda x: cosine(np.mean([w.vector for w in x], axis=0), input_vec),\n",
    "                  reverse=True)[:n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the sentences in *Dracula* closest in meaning to \"My favorite food is strawberry ice cream.\" (Extra linebreaks are present because we didn't strip them out when we originally read in the source text.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This, with some cheese\r\n",
      "and a salad and a bottle of old Tokay, of which I had two glasses, was\r\n",
      "my supper.\n",
      "---\n",
      "I got a cup of tea at the Aërated Bread Company\r\n",
      "and came down to Purfleet by the next train.\r\n",
      "\r\n",
      "\n",
      "---\n",
      "We get hot soup, or coffee, or tea; and\r\n",
      "off we go.\n",
      "---\n",
      "There is not even a toilet glass on my\r\n",
      "table, and I had to get the little shaving glass from my bag before I\r\n",
      "could either shave or brush my hair.\n",
      "---\n",
      "My own heart grew cold as ice,\r\n",
      "and I could hear the gasp of Arthur, as we recognised the features of\r\n",
      "Lucy Westenra.\n",
      "---\n",
      "I dined on what they\r\n",
      "called \"robber steak\"--bits of bacon, onion, and beef, seasoned with red\r\n",
      "pepper, and strung on sticks and roasted over the fire, in the simple\r\n",
      "style of the London cat's meat!\n",
      "---\n",
      "I believe they went to the trouble of putting an\r\n",
      "extra amount of garlic into our food; and I can't abide garlic.\n",
      "---\n",
      "Drink it off, like a good\r\n",
      "child.\n",
      "---\n",
      "I had for dinner, or\r\n",
      "rather supper, a chicken done up some way with red pepper, which was\r\n",
      "very good but thirsty.\n",
      "---\n",
      "I left Quincey lying down\r\n",
      "after having a glass of wine, and told the cook to get ready a good\r\n",
      "breakfast.\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "for sent in spacy_closest_sent(sentences, \"My favorite food is strawberry ice cream.\"):\n",
    "    print sent.text\n",
    "    print \"---\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further resources\n",
    "\n",
    "* [Word2vec](https://en.wikipedia.org/wiki/Word2vec) is another procedure for producing word vectors which uses a predictive approach rather than a context-counting approach. [This paper](http://clic.cimec.unitn.it/marco/publications/acl2014/baroni-etal-countpredict-acl2014.pdf) compares and contrasts the two approaches. (Spoiler: it's kind of a wash.)\n",
    "* If you want to train your own word vectors on a particular corpus, the popular Python library [gensim](https://radimrehurek.com/gensim/) has an implementation of Word2Vec that is relatively easy to use. [There's a good tutorial here.](https://rare-technologies.com/word2vec-tutorial/)\n",
    "* When you're working with vector spaces with high dimensionality and millions of vectors, iterating through your entire space calculating cosine similarities can be a drag. I use [Annoy](https://pypi.python.org/pypi/annoy) to make these calculations faster, and you should consider using it too."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
